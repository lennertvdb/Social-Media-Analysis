##############################
##############################
##### 1. Topic modeling  #####
##############################
##############################

#0_Set working directory, load the required packages, and clear the environment
setwd("c:/Users/vando/OneDrive/Documenten/Data Science 4 Business/2_Social Media and Web Analytics/Assignment/Social_media_group12/code/")

if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(rtweet, httr,tidyverse, reshape2, wordcloud, tm, topicmodels, topicdoc, tidytext, textclean, ggwordcloud)

rm(list=ls())

#1_load the required data
df_tweets <- read_csv("./../data/tweets_concat_removedScrapeDependency.csv")

#1.1_Small exploration of the data
head(df_tweets, 2)
head(df_tweets$text, 2)
dim(df_tweets)

colSums(is.na(df_tweets))/nrow(df_tweets) ; sum(is.na(df_tweets)) 

#2_Preprocessing
#2.1_basic textcleaning
tweets_text <- df_tweets %>% select(text) 

text_clean <- tweets_text$text %>%  
  str_to_lower() %>% # to lower case 
  str_replace_all("[[:punct:]]","") %>% # replace punctuation 
  str_replace_all("russiaukrainewar", "") %>%
  str_replace_all("ukraine", "") %>%
  str_replace_all("russia", "") %>%
  str_replace_all("russian", "") %>%
  str_replace_all("ukrainerussiawar", "") %>%
  str_replace_all("war", "") %>%
  str_replace_all("kyiv", "") %>%
  str_replace_all("0001f1fa", "") %>%
  str_replace_all("0001f1e6", "") %>%
  str_replace_all("0001f1f7", "") %>%
  str_replace_all("ukrainian", "") %>%
  str_replace_all("fe0f", "") %>%
  str_replace_all("amp", "") %>%
  str_replace_all("putin", "") %>%
  str_replace_all("rusukrwar", "") %>%
  str_replace_all("ww3", "") %>%
  str_replace_all("<u+>", "") %>%
  str_replace_all("world", "") %>%
  str_replace_all("stop", "") %>%
  str_replace_all("[[:digit:]]", "") %>% # remove numbers 
  removeWords(words = stopwords()) %>%  #remove stopwords
  str_squish() %>% # remove whitespace 
  str_replace_all("ff", "") %>% 
  str_replace_all("fe", "") %>% 
  str_replace_all("fd", "")

head(text_clean) 
length(text_clean)

#2.2_Create a document by term matrix
text_df <- tibble(doc= 1:length(text_clean), text = text_clean)

freq <- text_df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(doc,word, name = "freq", sort = TRUE)

dtm <- freq %>%
  cast_dtm(doc, word, freq)

dtm


#2.3_decide upon the optimal number of topics k
#2.3.1_iterate of multiple values: grid-search
ldas <- list()
j <- 0
for (i in 2:10) {
  j <- j+1
  print(i)
  ldas[[j]] <- LDA(x = dtm, k = i, control = list(seed = 1234))
}

#2.3.2_Use the AIC to determine the number of topics (lowest AIC)
(AICs <- data.frame(k = 2:5, aic = map_dbl(ldas, AIC)))
(K <- AICs$k[which.min(AICs$aic)])

#2.4_create the LDA model to eventually get the k topics
topicmodel <- LDA(x = dtm, k = k, control = list(seed = 1234))

#2.4.1_have a look at the probabilities that words are part of a certain topic
(topic_term <- tidy(topicmodel, matrix = 'beta')) #length = terms in dtm (332 828) * 2(#topics)

#2.4.2_top ten terms per topic
#2.4.2.1_create a list
top_terms <- topic_term %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta))
top_terms

#2.4.2.2_Visualize it with a plot
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#2.4.3_create wordclouds per topic

Select_terms <- topic_term %>% 
  group_by(topic) %>% 
  top_n(50, beta)

set.seed(42)# remember to make the plot window large enough!!!!!
ggplot(Select_terms, aes(label = term, size = beta, color = topic, x = topic)) +
  geom_text_wordcloud(area_corr = TRUE) +
  scale_size_area(max_size = 30) +
  theme_minimal()

#2.5_examine the relationship between topics and documents
#2.5.1_create the document per topic matrix
(doc_topic <- tidy(topicmodel, matrix = 'gamma')) #gamma == prob of words generated by topic k
# length : 590,446 documents (= 295 223 (#documents)* 2(#topics))

#2.5.2_Get the top topic for each document
(topics_gamma <- doc_topic %>% arrange(desc(gamma)))

#take the topic with highest probability
user_topic <- topics_gamma %>%
  group_by(document) %>%
  top_n(1, gamma)
user_topic #back to 295 223 documents

#2.5.3_add the original tweets to discover more about the topics
user_topic_tweet <- user_topic %>% 
  add_column(Tweets = tweets_text$text[as.numeric(user_topic$document)])
user_topic_tweet %>% slice_head() #295 223 observations


user_topic_tweet <- rename(user_topic_tweet, "text" = Tweets)
head(user_topic_tweet)
final_document_topics <- df_tweets %>% left_join(user_topic_tweet, by = "text")
View(final_document_topics)
dim(final_document_topics)
dim(df_tweets)

df_topic_documents <- final_document_topics %>% select(-c(document))
View(df_topic_documents)
dim(df_topic_documents)
df_topic_documents_unique <- df_topic_documents %>% distinct()
dim(df_topic_documents_unique)
dim(df_tweets) #so exactly 2 columns added


colSums(is.na(df_topic_documents_unique))/nrow(df_topic_documents_unique) ; sum(is.na(df_topic_documents_unique)) 

save(df_topic_documents_unique, file = "./../data/df_withTopics.RData")

setdiff(df_topic_documents_unique$text, df_tweets$text)
setdiff(df_tweets$text, df_topic_documents_unique$text)