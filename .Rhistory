source('tokensandkeys.R')
source('tokensandkeys')
source('tokensandkeys.R')
source('tokensandkeys.R')
token
#Install and load packages to retrieve network data from Twitter
if (!require("pacman")) install.packages("pacman", quiet=TRUE);require("pacman")
source('tokensandkeys.R')
token
p_load(rtweet, httpuv)
source('tokensandkeys.R')
token
?source
source("tokensandkeys.R")
StudentsNetwork <- data.frame(from=c('A','A','A','A','B','B','C','C','D','D','D', 'E','F','F','G','G','H','H','I'),
to=c('B','C','D','E','C','D','D','G','E','F', 'G','F','G','I','I','H','I','J','J'),
label=c(rep('dd',7),'do','dd','dd','do','dd','do','do',rep('oo',5)))
?rep
StudentsNetwork <- data.frame(from=c('Tom','Tom','Tom','Tom','Tom','Arno', 'Arno','Arno','Arno','Sofie', 'Sofie', 'Sofie', 'Jan', 'Jan', 'Karen'),
to=c('Arno', 'Sofie','Jan','Karen', 'Laura', 'Sofie', 'Jan', 'Karen', 'Laura','Jan', 'Karen', 'Laura', 'Karen', 'Laura', 'Laura'),
label=c(rep('bb',2),rep('be', 3),'bb', rep('be', 3), rep('be',3), rep('ee',3)))
StudentsNetwork <- data.frame(from=c("Tom", "Sofie", "Karen"),
to=c("Arno", "Jan", "Laura"),
label=c("bb", "be", "ee"))
StudentsNetwork
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "dd", "blue", ifelse(StudentsNetwork$label == "do", "red", "darkgreen")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == "DA", "blue", "darkgreen"),
vertex.label.color = 'white')
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
V(g)$master <- as.character(Students$master)
layout <- layout_nicely(g)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "dd", "blue", ifelse(StudentsNetwork$label == "do", "red", "darkgreen")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == "DA", "blue", "darkgreen"),
vertex.label.color = 'white')
p_load(igraph)
library(pacman)
p_load(igraph)
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
V(g)$master <- as.character(Students$master)
layout <- layout_nicely(g)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "dd", "blue", ifelse(StudentsNetwork$label == "do", "red", "darkgreen")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == "DA", "blue", "darkgreen"),
vertex.label.color = 'white')
StudentsNetwork <- data.frame(from=c("Tom", "Sofie", "Karen"),
to=c("Arno", "Jan", "Laura"),
label=c("bb", "be", "ee"))
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
V(g)$master <- as.character(Students$master)
layout <- layout_nicely(g)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue", ifelse(StudentsNetwork$label == "be", "red", "darkgreen")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == "be", "blue", "darkgreen"),
vertex.label.color = 'white')
## Connectedness
p <- 2*M/(N*(N-1))
## Number of nodes
N <- length(V(g))
n_b <- length(which(V(g)$master=='B'))
n_e <- length(which(V(g)$master=='E'))
## Number of edges
M <- length(E(g))
m_dd <- length(which(E(g)$label=='bb'))
m_oo <- length(which(E(g)$label=='ee'))
m_do <- length(which(E(g)$label=='be'))
## Connectedness
p <- 2*M/(N*(N-1))
p
#the same as:
graph.density(g)
bar_m_do <- n_da*n_or*p
H <- m_do/bar_m_do
H
# 4. Heterophilicity
bar_m_do <- n_da*n_or*p
# 4. Heterophilicity
bar_m_be <- n_b*n_e*p
H <- m_be/bar_m_be
m_be <- length(which(E(g)$label=='be'))
H <- m_be/bar_m_be
H
# 4. Heterophilicity
bar_m_be <- n_b*n_e*p
H <- m_be/bar_m_be
H
bar_m_bb <- n_be*(n_be-1)/2*p
D_bb <- m_bb/bar_m_bb
D_bb
# 5. Dyadicity
bar_m_bb <- n_be*(n_be-1)/2*p
bar_m_bb <- n_b*(n_b-1)/2*p
D_bb <- m_bb/bar_m_bb
D_bb
# 5. Dyadicity
bar_m_bb <- n_b*(n_b-1)/2*p
D_bb <- m_bb/bar_m_bb
m_bb <- length(which(E(g)$label=='bb'))
D_bb <- m_bb/bar_m_bb
D_bb
#7. Graph Representation Learning
p_load(node2vec)
str(StudentsNetwork)
emb <- node2vecR(StudentsNetwork[,-3], p=1,q=1,num_walks=5,walk_length=5,dim=5)
emb_name <- data.frame(node = row.names(emb), emb)
head(sort(cos_sim[,1], decreasing = TRUE), 10)
cos_sim <- sim2(x = as.matrix(emb), y = as.matrix(emb[emb_name$node == "G",, drop = FALSE]), method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
emb <- node2vecR(StudentsNetwork[,-3], p=1,q=1,num_walks=5,walk_length=5,dim=5)
emb_name <- data.frame(node = row.names(emb), emb)
cos_sim <- sim2(x = as.matrix(emb), y = as.matrix(emb[emb_name$node == "G",, drop = FALSE]), method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
library(pacman)
p_load(node2vec)
emb <- node2vecR(StudentsNetwork[,-3], p=1,q=1,num_walks=5,walk_length=5,dim=5)
..$ : NULL
emb_name <- data.frame(node = row.names(emb), emb)
head(sort(cos_sim[,1], decreasing = TRUE), 10)
rm(list=ls())
rm(list=ls())
#1. Scraping
library(pacman)
p_load(rvest)
vrt_url <- "https://www.vrt.be/vrtnws/nl/rubrieken/desinformatie/"
html <- vrt_url %>% read_html()
html
length(nodes)
nodes <- html %>% html_nodes('.vrt-teaser__title-text')
length(nodes)
articles <- nodes %>% html_text()
#2.1 Social Network Learning - Plotting
detach(package:statnet, unload=TRUE)
p_load(igraph)
BankNetwork <- data.frame(
from = c('A', 'A', 'A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'D', 'E',
'F', 'F', 'G', 'G', 'H', 'H', 'I'),
to = c('B','C','D','E','C','D','D', 'G','E', 'F','G','F','G','I',
'I','H','I','J','J'))
g <- graph_from_data_frame(BankNetwork, directed = FALSE)
plot.igraph(g, edge.label = NA,
edge.color = 'black',
layout = layout,
vertex.label = V(g)$name, vertex.color = 'white',
vertex.label.color = 'black')
#1. Scraping
library(pacman)
p_load(rvest)
vrt_url <- "https://www.vrt.be/vrtnws/nl/rubrieken/desinformatie/"
html <- vrt_url %>% read_html()
nodes <- html %>% html_nodes('.vrt-teaser__title-text')
length(nodes)
articles <- nodes %>% html_text()
#2.1 Social Network Learning - Plotting
detach(package:statnet, unload=TRUE)
p_load(igraph)
BankNetwork <- data.frame(
from = c('A', 'A', 'A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'D', 'E',
'F', 'F', 'G', 'G', 'H', 'H', 'I'),
to = c('B','C','D','E','C','D','D', 'G','E', 'F','G','F','G','I',
'I','H','I','J','J'))
g <- graph_from_data_frame(BankNetwork, directed = FALSE)
layout <- layout_nicely(g)
plot.igraph(g, edge.label = NA,
edge.color = 'black',
layout = layout,
vertex.label = V(g)$name, vertex.color = 'white',
vertex.label.color = 'black')
plot.igraph(g, edge.label = NA, edge.color = 'black', layout = layout,
vertex.label = V(g)$name, vertex.color = V(g)$color,
vertex.label.color = 'black')
#1. Scraping
library(pacman)
p_load(rvest)
vrt_url <- "https://www.vrt.be/vrtnws/nl/rubrieken/desinformatie/"
html <- vrt_url %>% read_html()
nodes <- html %>% html_nodes('.vrt-teaser__title-text')
length(nodes)
articles <- nodes %>% html_text()
#2.1 Social Network Learning - Plotting
detach(package:statnet, unload=TRUE)
p_load(igraph)
BankNetwork <- data.frame(
from = c('A', 'A', 'A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'D', 'E',
'F', 'F', 'G', 'G', 'H', 'H', 'I'),
to = c('B','C','D','E','C','D','D', 'G','E', 'F','G','F','G','I',
'I','H','I','J','J'))
g <- graph_from_data_frame(BankNetwork, directed = FALSE)
layout <- layout_nicely(g)
plot.igraph(g, edge.label = NA,
edge.color = 'black',
layout = layout,
vertex.label = V(g)$name, vertex.color = 'white',
vertex.label.color = 'black')
V(g)$churn <- c(1,1,1,1,0,0,NA,0,1,0)
V(g)$color <- ifelse(is.na(V(g)$churn), 'grey', ifelse(
V(g)$churn == 1 , 'red', 'green'))
plot.igraph(g, edge.label = NA, edge.color = 'black', layout = layout,
vertex.label = V(g)$name, vertex.color = V(g)$color,
vertex.label.color = 'black')
#2.1 Social Network Learning - Plotting
detach(package:statnet, unload=TRUE)
p_load(igraph)
PadelNetwork <- data.frame(
from = c('1', '1', '1', '1', '2', '3', '3', '4', '4', '4', '5', '5',
'5', '7', '8', '9'),
to = c('3','4','5','2','8','6','4', '6','10', '5','7','8','9','9',
'9','10'))
n <- graph_from_data_frame(BankNetwork, directed = FALSE)
layout <- layout_nicely(n)
plot.igraph(n, edge.label = NA,
edge.color = 'black',
layout = layout,
vertex.label = V(n)$name, vertex.color = 'white',
vertex.label.color = 'black')
V(n)$gender <- c(1,1,1,1,0,0,NA,0,1,0)
V(n)$color <- ifelse(is.na(V(n)$gender), 'grey', ifelse(
V(n)$gender == 1 , 'dodgerblue', 'white'))
plot.igraph(n, edge.label = NA, edge.color = 'black', layout = layout,
vertex.label = V(n)$name, vertex.color = V(n)$color,
vertex.label.color = 'black')
V(n)$gender <- c(1,0,0,1,0,0,1,0,1,0)
V(n)$color <- ifelse(is.na(V(n)$gender), 'grey', ifelse(
V(n)$gender == 1 , 'dodgerblue', 'white'))
plot.igraph(n, edge.label = NA, edge.color = 'black', layout = layout,
vertex.label = V(n)$name, vertex.color = V(n)$color,
vertex.label.color = 'black')
PadelNetwork <- data.frame(
from = c('1', '1', '1', '1', '2', '3', '3', '4', '4', '4', '5', '5',
'5', '7', '8', '9'),
to = c('3','4','5','2','8','6','4', '6','10', '5','7','8','9','9',
'9','10'))
n <- graph_from_data_frame(BankNetwork, directed = FALSE)
layout <- layout_nicely(n)
plot.igraph(n, edge.label = NA,
edge.color = 'black',
layout = layout,
vertex.label = V(n)$name, vertex.color = 'white',
vertex.label.color = 'black')
V(n)$gender <- c(1,0,0,1,0,0,1,0,1,0)
V(n)$color <- ifelse(is.na(V(n)$gender), 'grey', ifelse(
V(n)$gender == 1 , 'dodgerblue', 'white'))
plot.igraph(n, edge.label = NA, edge.color = 'black', layout = layout,
vertex.label = V(n)$name, vertex.color = V(n)$color,
vertex.label.color = 'black')
n <- graph_from_data_frame(PadelNetwork, directed = FALSE)
layout <- layout_nicely(n)
plot.igraph(n, edge.label = NA,
edge.color = 'black',
layout = layout,
vertex.label = V(n)$name, vertex.color = 'white',
vertex.label.color = 'black')
V(n)$gender <- c(1,0,0,1,0,0,1,0,1,0)
V(n)$color <- ifelse(is.na(V(n)$gender), 'grey', ifelse(
V(n)$gender == 1 , 'dodgerblue', 'white'))
plot.igraph(n, edge.label = NA, edge.color = 'black', layout = layout,
vertex.label = V(n)$name, vertex.color = V(n)$color,
vertex.label.color = 'black')
detach(package:statnet, unload=TRUE)
p_load(igraph)
PadelNetwork <- data.frame(
from = c("1", "1", "1", "1", "2", "3", "4", "4", "4", "5", "5", "5", "6", "7", "8", "9"),
to = c("2", "3", "5", "4", "8", "4", "5", "6", "10", "7", "8", "9", "3", "9", "9", "10"))
n <- graph_from_data_frame(PadelNetwork, directed = FALSE)
layout <- layout_nicely(n)
plot.igraph(n, edge.label = NA,
edge.color = 'black',
layout = layout,
vertex.label = V(n)$name, vertex.color = 'white',
vertex.label.color = 'black')
V(n)$gender <- c(1,0,0,1,0,0,1,0,1,0)
V(n)$color <- ifelse(is.na(V(n)$gender), 'grey', ifelse(
V(n)$gender == 1 , 'blue', 'white'))
plot.igraph(n, edge.label = NA, edge.color = 'black', layout = layout,
vertex.label = V(n)$name, vertex.color = V(n)$color,
vertex.label.color = 'black')
#Relational Neighbor Classifier
p_load(sna)
#Relational Neighbor Classifier
p_load(sna)
net <- network(PadelNetwork,matrix.type="edgelist", directed = FALSE)
adjacency <- as.sociomatrix(net)
#Relational Neighbor Classifier
p_load(sna)
net <- network(PadelNetwork,matrix.type="edgelist", directed = FALSE)
adjacency <- as.sociomatrix(net)
male_neighbors <- numeric(length = ncol(adjacency))
female_neighbors <- numeric(length = ncol(adjacency))
for (i in 1:ncol(adjacency)) {
neighbors <- which(adjacency[,i]==1)
male_neighbors[i] <- sum(V(n)$churn[neighbors], na.rm = TRUE)
female_neighbors[i] <- sum(V(n)$churn[neighbors] == 0, na.rm = TRUE)
}
p_load(sna)
net <- network(PadelNetwork,matrix.type="edgelist", directed = FALSE)
adjacency <- as.sociomatrix(net)
male_neighbors <- numeric(length = ncol(adjacency))
female_neighbors <- numeric(length = ncol(adjacency))
for (i in 1:ncol(adjacency)) {
neighbors <- which(adjacency[,i]==1)
male_neighbors[i] <- sum(V(n)$churn[neighbors], na.rm = TRUE)
female_neighbors[i] <- sum(V(n)$churn[neighbors] == 0, na.rm = TRUE)
}
prob_male <- male_neighbors / (male_neighbors + female_neighbors)
data.frame(rownames(adjacency),prob_male)
data.frame(nodes,prob_male)
prob_female <- female_neighbors / (male_neighbors + female_neighbors)
rownames(adjacency)
## ------------------Lecture 5: Sentiment Analysis Dodona------------------ ##
rm(list=ls())
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(httr,rtweet,tidyverse,readr,skimr)
# load packages
library(tidyverse)
library(stringr)
library(lubridate)
# define hashtags
hashtags_selected <- c("#putin","#RusUkrWar","#stopwar","#ww3")
for (i in 1:length(hashtags_selected)){
# get the paths and filenames
path_to_data <- str_c("C:/Users/Lenne/Desktop/Social_media_group12/data" ,hashtags_selected[i])
files <- list.files(path_to_data)
# concat tweets of same hashtag
for (j in 1:length(files)){
print(paste(i,j))
# load twitter df
tweets_loaded <- read_csv(str_c(path_to_data,"/",files[j]), show_col_types = FALSE)
# check if actual data in file, otherwise continue
if (nrow(tweets_loaded) == 0){
print(paste("No data in file: ",files[j]))
next
}
# add scrape hashtag to df
tweets_loaded <- tweets_loaded %>% mutate(hashtag_scrape = hashtags_selected[i])
# add day of scrape to df
date_scrape <- str_match(files[j],"\\w+(\\d{4}-\\d{2}-\\d{2})\\.csv")[2]
tweets_loaded <- tweets_loaded %>% mutate(date_scrape = date_scrape)
# get booleans if media and url or not
tweets_loaded <- tweets_loaded %>% mutate(url = !is.na(urls_url)) %>%
mutate(media = !is.na(media_url))
# select relevant columns
cols_relevant <- c("date_scrape","created_at","text","media","url","favorite_count","retweet_count","hashtags","hashtag_scrape","screen_name","followers_count","friends_count","verified")
tweets_loaded <- tweets_loaded %>% select(cols_relevant)
# append df's row-wise in correct way
if (j == 1){
tweets_hashtag <- tweets_loaded
}else{
tweets_hashtag <- rbind(tweets_hashtag, tweets_loaded)
}
}
# concat tweets of different hashtags
if (i == 1){
tweets <- tweets_hashtag
}else{
tweets <- rbind(tweets,tweets_hashtag)
}
}
# remove scrape dependent information of tweet (EXCEPTION for FOLLOWERS_COUNT and FRIENDS_COUNT)
cols_relevant <- c("created_at","text","media","url","hashtags","hashtag_scrape","screen_name","followers_count","friends_count","verified")
tweets_unique <- tweets %>% select(cols_relevant) %>% unique() %>% arrange(created_at,text)
tweets_unique_grouped <- tweets_unique %>%
group_by(created_at,text,media,url,hashtags,hashtag_scrape,screen_name,verified) %>%
summarise(followers_count = mean(followers_count),friends_count = mean(friends_count))
# save tweet df
write.csv(tweets_unique_grouped,"C:/Users/Lenne/Desktop/Social_media_group12/data/tweets_concat.csv", row.names = FALSE)
# load tweets again as check
test <- read.csv("./../data/tweets_concat.csv")
#------Clean the environment----------
rm(list=ls())
#------Install and load packages------
if (!require('pacman')) install.packages('pacman') ; require('pacman')
p_load(tidyverse, keras, caret, ggplot2)
#------Setup working directory------
setwd('C:/Users/Lenne/Desktop/Social_media_group12')
#------Loading data------
load('C:/Users/Lenne/Desktop/Social_media_group12/data/basetable_final.RData')
#------Exploring data------
#Price
price <- basetable %>%
ggplot(aes(x = time,
y = price_stock))
price + geom_line(size = 0.5)
#Volume
volume <- basetable %>%
ggplot(aes(x = time,
y = volume_stock))
volume + geom_line(size = 0.5)
#------Recurrent Neural Network------
set.seed(123)
##---Set model parameters
max_len <- 6
batch_size <- 32
total_epochs <- 15
# get a list of start indexes for our (overlapping) chunks
start_indexes <- seq(1, length(basetable) - (max_len + 1), by = 3)
# create an empty matrix to store our data in
stock_matrix <- matrix(nrow = length(start_indexes), ncol = max_len + 1)
# fill our matrix with the overlapping slices of our dataset
for (i in 1:length(start_indexes)){
stock_matrix[i,] <- basetable[start_indexes[i]:(start_indexes[i] + max_len)]
}
# remove na's, just to be sure
if(anyNA(stock_matrix)){
stock_matrix <- na.omit(stock_matrix)
}
#------------ RNN for stock prediction ----------------
#------Clean the environment----------
rm(list=ls())
#------Install and load packages------
if (!require('pacman')) install.packages('pacman') ; require('pacman')
p_load(tidyverse, keras, caret, ggplot2)
#------Setup working directory------
setwd('C:/Users/Lenne/Desktop/Social_media_group12')
#------Loading data------
load('C:/Users/Lenne/Desktop/Social_media_group12/data/basetable_final.RData')
#------Exploring data------
#Price
price <- basetable %>%
ggplot(aes(x = time,
y = price_stock))
price + geom_line(size = 0.5)
#Volume
volume <- basetable %>%
ggplot(aes(x = time,
y = volume_stock))
volume + geom_line(size = 0.5)
#------Recurrent Neural Network------
set.seed(123)
##---Set model parameters
max_len <- 6
batch_size <- 32
total_epochs <- 15
max_features <- 10000
##---Sub sampling technique
#-As we have not much observations to train a deep learning
# model we will stretch out our data, we can use something called
# moving-block sub-sampling, which is where we cut up our vector into
# overlapping chunks.
#-IMPORTANT: we care about the order that our observations are in and
# want to make sure it's preserved.
# Cut the text in overlapping sample sequences of max_len characters
# get a list of start indexes for our (overlapping) chunks
start_indexes <- seq(1, length(basetable) - (max_len + 1), by = 3)
# create an empty matrix to store our data in
stock_matrix <- matrix(nrow = length(start_indexes), ncol = max_len + 1)
# fill our matrix with the overlapping slices of our dataset
for (i in 1:length(start_indexes)){
stock_matrix[i,] <- basetable[start_indexes[i]:(start_indexes[i] + max_len)]
}
# remove na's, just to be sure
if(anyNA(stock_matrix)){
stock_matrix <- na.omit(stock_matrix)
stock_matrix
View(stock_matrix)
X <- basetable[,-ncol(basetable)]
y <- basetable[,ncol(basetable)]
training_index <- createDataPartition(y, p = .9,
list = FALSE,
times = 1)
X_train <- array(X[training_index,], dim = c(length(training_index), max_len, 5))
y_train <- y[training_index]
X_test <- array(X[-training_index,], dim = c(length(y) - length(training_index), max_len, 5))
y_test <- y[-training_index]
model <- keras_model_sequential()
dim(X_train)
X_train
# our input layer
model %>%
layer_dense(input_shape = dim(X_train)[2:3], units = max_len)
#- Hidden layer
model %>%
layer_simple_rnn(units = 6)
#- Output layer
model %>%
layer_dense(units = 1, activation = 'linear')
##--- Model Architecture
summary(model)
model %>% compile(loss = 'MSE',
optimizer = 'RMSprop',
metrics = c("acc"))
#--- Training our model
trained_model <- model %>% fit(
x = X_train, # sequence we're using for prediction
y = y_train, # sequence we're predicting
batch_size = batch_size, # how many samples to pass to our model at a time
epochs = total_epochs, # how many times we'll look @ the whole dataset
validation_split = 0.1) # how much data to hold out for testing as we go along
plot(trained_model)
#--- Evaluate model
model %>% evaluate(X_test, y_test, batch_size = batch_size)
#------Clean the environment----------
rm(list=ls())
#------Clean the environment----------
rm(list=ls())
#------Clean the environment----------
rm(list=ls())
#------Install and load packages------
if (!require('pacman')) install.packages('pacman') ; require('pacman')
#------Clean the environment----------
rm(list=ls())
